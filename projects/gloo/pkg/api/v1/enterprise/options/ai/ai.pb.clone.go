// Code generated by protoc-gen-ext. DO NOT EDIT.
// source: github.com/solo-io/gloo/projects/gloo/api/v1/enterprise/options/ai/ai.proto

package ai

import (
	"bytes"
	"encoding/binary"
	"errors"
	"fmt"
	"strings"

	"github.com/solo-io/protoc-gen-ext/pkg/clone"
	"google.golang.org/protobuf/proto"
)

// ensure the imports are used
var (
	_ = errors.New("")
	_ = fmt.Print
	_ = binary.LittleEndian
	_ = bytes.Compare
	_ = strings.Compare
	_ = clone.Cloner(nil)
	_ = proto.Message(nil)
)

// Clone function
func (m *UpstreamSpec) Clone() proto.Message {
	var target *UpstreamSpec
	if m == nil {
		return target
	}
	target = &UpstreamSpec{}

	switch m.AuthToken.(type) {

	case *UpstreamSpec_InlineAuthToken:

		target.AuthToken = &UpstreamSpec_InlineAuthToken{
			InlineAuthToken: m.GetInlineAuthToken(),
		}

	case *UpstreamSpec_AuthTokenRef:

		target.AuthToken = &UpstreamSpec_AuthTokenRef{
			AuthTokenRef: m.GetAuthTokenRef(),
		}

	}

	switch m.Llm.(type) {

	case *UpstreamSpec_Openai:

		if h, ok := interface{}(m.GetOpenai()).(clone.Cloner); ok {
			target.Llm = &UpstreamSpec_Openai{
				Openai: h.Clone().(*UpstreamSpec_OpenAI),
			}
		} else {
			target.Llm = &UpstreamSpec_Openai{
				Openai: proto.Clone(m.GetOpenai()).(*UpstreamSpec_OpenAI),
			}
		}

	case *UpstreamSpec_Mistral_:

		if h, ok := interface{}(m.GetMistral()).(clone.Cloner); ok {
			target.Llm = &UpstreamSpec_Mistral_{
				Mistral: h.Clone().(*UpstreamSpec_Mistral),
			}
		} else {
			target.Llm = &UpstreamSpec_Mistral_{
				Mistral: proto.Clone(m.GetMistral()).(*UpstreamSpec_Mistral),
			}
		}

	case *UpstreamSpec_Custom_:

		if h, ok := interface{}(m.GetCustom()).(clone.Cloner); ok {
			target.Llm = &UpstreamSpec_Custom_{
				Custom: h.Clone().(*UpstreamSpec_Custom),
			}
		} else {
			target.Llm = &UpstreamSpec_Custom_{
				Custom: proto.Clone(m.GetCustom()).(*UpstreamSpec_Custom),
			}
		}

	}

	return target
}

// Clone function
func (m *RouteSettings) Clone() proto.Message {
	var target *RouteSettings
	if m == nil {
		return target
	}
	target = &RouteSettings{}

	if h, ok := interface{}(m.GetPromptEnrichment()).(clone.Cloner); ok {
		target.PromptEnrichment = h.Clone().(*AIPromptEnrichment)
	} else {
		target.PromptEnrichment = proto.Clone(m.GetPromptEnrichment()).(*AIPromptEnrichment)
	}

	if h, ok := interface{}(m.GetPromptGuard()).(clone.Cloner); ok {
		target.PromptGuard = h.Clone().(*AIPromptGaurd)
	} else {
		target.PromptGuard = proto.Clone(m.GetPromptGuard()).(*AIPromptGaurd)
	}

	if h, ok := interface{}(m.GetRateLimiting()).(clone.Cloner); ok {
		target.RateLimiting = h.Clone().(*RateLimiting)
	} else {
		target.RateLimiting = proto.Clone(m.GetRateLimiting()).(*RateLimiting)
	}

	if h, ok := interface{}(m.GetRag()).(clone.Cloner); ok {
		target.Rag = h.Clone().(*RAG)
	} else {
		target.Rag = proto.Clone(m.GetRag()).(*RAG)
	}

	if h, ok := interface{}(m.GetSemanticCaching()).(clone.Cloner); ok {
		target.SemanticCaching = h.Clone().(*SemanticCaching)
	} else {
		target.SemanticCaching = proto.Clone(m.GetSemanticCaching()).(*SemanticCaching)
	}

	if m.GetBackupModels() != nil {
		target.BackupModels = make([]string, len(m.GetBackupModels()))
		for idx, v := range m.GetBackupModels() {

			target.BackupModels[idx] = v

		}
	}

	return target
}

// Clone function
func (m *Postgres) Clone() proto.Message {
	var target *Postgres
	if m == nil {
		return target
	}
	target = &Postgres{}

	target.ConnectionString = m.GetConnectionString()

	target.CollectionName = m.GetCollectionName()

	return target
}

// Clone function
func (m *Redis) Clone() proto.Message {
	var target *Redis
	if m == nil {
		return target
	}
	target = &Redis{}

	target.ConnectionString = m.GetConnectionString()

	return target
}

// Clone function
func (m *Embedding) Clone() proto.Message {
	var target *Embedding
	if m == nil {
		return target
	}
	target = &Embedding{}

	switch m.Embedding.(type) {

	case *Embedding_Openai:

		if h, ok := interface{}(m.GetOpenai()).(clone.Cloner); ok {
			target.Embedding = &Embedding_Openai{
				Openai: h.Clone().(*Embedding_OpenAI),
			}
		} else {
			target.Embedding = &Embedding_Openai{
				Openai: proto.Clone(m.GetOpenai()).(*Embedding_OpenAI),
			}
		}

	}

	return target
}

// Clone function
func (m *SemanticCaching) Clone() proto.Message {
	var target *SemanticCaching
	if m == nil {
		return target
	}
	target = &SemanticCaching{}

	if h, ok := interface{}(m.GetDatastore()).(clone.Cloner); ok {
		target.Datastore = h.Clone().(*SemanticCaching_DataStore)
	} else {
		target.Datastore = proto.Clone(m.GetDatastore()).(*SemanticCaching_DataStore)
	}

	if h, ok := interface{}(m.GetEmbedding()).(clone.Cloner); ok {
		target.Embedding = h.Clone().(*Embedding)
	} else {
		target.Embedding = proto.Clone(m.GetEmbedding()).(*Embedding)
	}

	target.Ttl = m.GetTtl()

	return target
}

// Clone function
func (m *RAG) Clone() proto.Message {
	var target *RAG
	if m == nil {
		return target
	}
	target = &RAG{}

	if h, ok := interface{}(m.GetDatastore()).(clone.Cloner); ok {
		target.Datastore = h.Clone().(*RAG_DataStore)
	} else {
		target.Datastore = proto.Clone(m.GetDatastore()).(*RAG_DataStore)
	}

	if h, ok := interface{}(m.GetEmbedding()).(clone.Cloner); ok {
		target.Embedding = h.Clone().(*Embedding)
	} else {
		target.Embedding = proto.Clone(m.GetEmbedding()).(*Embedding)
	}

	target.PromptTemplate = m.GetPromptTemplate()

	return target
}

// Clone function
func (m *RateLimiting) Clone() proto.Message {
	var target *RateLimiting
	if m == nil {
		return target
	}
	target = &RateLimiting{}

	if m.GetRateLimitConfigs() != nil {
		target.RateLimitConfigs = make([]string, len(m.GetRateLimitConfigs()))
		for idx, v := range m.GetRateLimitConfigs() {

			target.RateLimitConfigs[idx] = v

		}
	}

	return target
}

// Clone function
func (m *AIPromptEnrichment) Clone() proto.Message {
	var target *AIPromptEnrichment
	if m == nil {
		return target
	}
	target = &AIPromptEnrichment{}

	if m.GetPrepend() != nil {
		target.Prepend = make([]*AIPromptEnrichment_Message, len(m.GetPrepend()))
		for idx, v := range m.GetPrepend() {

			if h, ok := interface{}(v).(clone.Cloner); ok {
				target.Prepend[idx] = h.Clone().(*AIPromptEnrichment_Message)
			} else {
				target.Prepend[idx] = proto.Clone(v).(*AIPromptEnrichment_Message)
			}

		}
	}

	if m.GetAppend() != nil {
		target.Append = make([]*AIPromptEnrichment_Message, len(m.GetAppend()))
		for idx, v := range m.GetAppend() {

			if h, ok := interface{}(v).(clone.Cloner); ok {
				target.Append[idx] = h.Clone().(*AIPromptEnrichment_Message)
			} else {
				target.Append[idx] = proto.Clone(v).(*AIPromptEnrichment_Message)
			}

		}
	}

	return target
}

// Clone function
func (m *AIPromptGaurd) Clone() proto.Message {
	var target *AIPromptGaurd
	if m == nil {
		return target
	}
	target = &AIPromptGaurd{}

	if h, ok := interface{}(m.GetRequest()).(clone.Cloner); ok {
		target.Request = h.Clone().(*AIPromptGaurd_Request)
	} else {
		target.Request = proto.Clone(m.GetRequest()).(*AIPromptGaurd_Request)
	}

	if h, ok := interface{}(m.GetResponse()).(clone.Cloner); ok {
		target.Response = h.Clone().(*AIPromptGaurd_Response)
	} else {
		target.Response = proto.Clone(m.GetResponse()).(*AIPromptGaurd_Response)
	}

	return target
}

// Clone function
func (m *UpstreamSpec_OpenAI) Clone() proto.Message {
	var target *UpstreamSpec_OpenAI
	if m == nil {
		return target
	}
	target = &UpstreamSpec_OpenAI{}

	return target
}

// Clone function
func (m *UpstreamSpec_Mistral) Clone() proto.Message {
	var target *UpstreamSpec_Mistral
	if m == nil {
		return target
	}
	target = &UpstreamSpec_Mistral{}

	return target
}

// Clone function
func (m *UpstreamSpec_Custom) Clone() proto.Message {
	var target *UpstreamSpec_Custom
	if m == nil {
		return target
	}
	target = &UpstreamSpec_Custom{}

	target.Host = m.GetHost()

	target.Port = m.GetPort()

	return target
}

// Clone function
func (m *Embedding_OpenAI) Clone() proto.Message {
	var target *Embedding_OpenAI
	if m == nil {
		return target
	}
	target = &Embedding_OpenAI{}

	return target
}

// Clone function
func (m *SemanticCaching_DataStore) Clone() proto.Message {
	var target *SemanticCaching_DataStore
	if m == nil {
		return target
	}
	target = &SemanticCaching_DataStore{}

	switch m.Datastore.(type) {

	case *SemanticCaching_DataStore_Redis:

		if h, ok := interface{}(m.GetRedis()).(clone.Cloner); ok {
			target.Datastore = &SemanticCaching_DataStore_Redis{
				Redis: h.Clone().(*Redis),
			}
		} else {
			target.Datastore = &SemanticCaching_DataStore_Redis{
				Redis: proto.Clone(m.GetRedis()).(*Redis),
			}
		}

	}

	return target
}

// Clone function
func (m *RAG_DataStore) Clone() proto.Message {
	var target *RAG_DataStore
	if m == nil {
		return target
	}
	target = &RAG_DataStore{}

	switch m.Datastore.(type) {

	case *RAG_DataStore_Postgres:

		if h, ok := interface{}(m.GetPostgres()).(clone.Cloner); ok {
			target.Datastore = &RAG_DataStore_Postgres{
				Postgres: h.Clone().(*Postgres),
			}
		} else {
			target.Datastore = &RAG_DataStore_Postgres{
				Postgres: proto.Clone(m.GetPostgres()).(*Postgres),
			}
		}

	}

	return target
}

// Clone function
func (m *AIPromptEnrichment_Message) Clone() proto.Message {
	var target *AIPromptEnrichment_Message
	if m == nil {
		return target
	}
	target = &AIPromptEnrichment_Message{}

	target.Role = m.GetRole()

	target.Content = m.GetContent()

	target.RoleOverride = m.GetRoleOverride()

	return target
}

// Clone function
func (m *AIPromptGaurd_Request) Clone() proto.Message {
	var target *AIPromptGaurd_Request
	if m == nil {
		return target
	}
	target = &AIPromptGaurd_Request{}

	if m.GetMatches() != nil {
		target.Matches = make([]string, len(m.GetMatches()))
		for idx, v := range m.GetMatches() {

			target.Matches[idx] = v

		}
	}

	if m.GetBuiltins() != nil {
		target.Builtins = make([]AIPromptGaurd_Request_BuiltIn, len(m.GetBuiltins()))
		for idx, v := range m.GetBuiltins() {

			target.Builtins[idx] = v

		}
	}

	target.CustomResponseMessage = m.GetCustomResponseMessage()

	return target
}

// Clone function
func (m *AIPromptGaurd_Response) Clone() proto.Message {
	var target *AIPromptGaurd_Response
	if m == nil {
		return target
	}
	target = &AIPromptGaurd_Response{}

	if m.GetMatches() != nil {
		target.Matches = make([]string, len(m.GetMatches()))
		for idx, v := range m.GetMatches() {

			target.Matches[idx] = v

		}
	}

	if m.GetBuiltins() != nil {
		target.Builtins = make([]AIPromptGaurd_Response_BuiltIn, len(m.GetBuiltins()))
		for idx, v := range m.GetBuiltins() {

			target.Builtins[idx] = v

		}
	}

	return target
}
