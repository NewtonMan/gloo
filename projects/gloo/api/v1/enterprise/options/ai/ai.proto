syntax = "proto3";
package ai.options.gloo.solo.io;
option go_package = "github.com/solo-io/gloo/projects/gloo/pkg/api/v1/enterprise/options/ai";

import "google/protobuf/wrappers.proto";
import "google/protobuf/struct.proto";
import "google/protobuf/duration.proto";
import "google/protobuf/empty.proto";
import "extproto/ext.proto";
option (extproto.equal_all) = true;
option (extproto.hash_all) = true;
option (extproto.clone_all) = true;

message SingleAuthToken {
  message SecretRef {
    // name of k8s secret in the same namesapce as the Upstream
    string name = 2;
    // Optional secret key to use
    string key = 3;
  }
  oneof auth_token_source {
    // Provide easy inline way to specify a token
    string inline = 1;
    // Reference to a secret in the same namespace as the Upstream
    SecretRef secret_ref = 2;
  }
}

/*
  The AI UpstreamSpec represents a logical LLM provider backend.
  The purpose of this spec is a way to configure which backend to use
  as well as how to authenticate with the backend.

  Currently the options are:
  - OpenAI
    * Default Host: api.openai.com
    * Default Port: 443
    * Auth Token: Bearer token to use for the OpenAI API
  - Mistral
    * Default Host: api.mistral.com
    * Default Port: 443
    * Auth Token: Bearer token to use for the Mistral API
  - Anthropic
    * Default Host: api.anthropic.com
    * Default Port: 443
    * Auth Token: x-api-key to use for the Anthropic API
    * Version: Optional version header to pass to the Anthropic API
  
  All of the above backends can be configured to use a custom host and port.
  This option is meant to allow users to proxy the request, or to use a different
  backend altogether which is API compliant with the upstream version.

  Examples:

  OpenAI with inline auth token:
  ```
  ai:
    openai:
      authToken:
        inline: "my_token"
  ```

  Mistral with secret ref:
  ```
  ai:
    mistral:
      authToken:
        secretRef:
          name: "my-secret"
          key: "my-key"
  ```

  Anthropic with inline token and custom Host:
  ```
  ai:
    anthropic:
      authToken:
        inline: "my_token"
      customHost:
        host: "my-anthropic-host.com"
        port: 443 # Port is optional and will default to 443 for HTTPS
  ```
*/
message UpstreamSpec {

  message CustomHost {
    // Custom host to send the traffic to
    string host = 1;
    // Custom host to send the traffic to
    uint32 port = 2;
  }

  // Settings for the OpenAI API
  message OpenAI {
    // Auth Token to use for the OpenAI API
    // This token will be placed into the `Authorization` header
    // and prefixed with Bearer if not present
    // when sending the request to the upstream.
    SingleAuthToken auth_token = 1;
    // Optional custom host to send the traffic to
    CustomHost custom_host = 2;
  }

  // Settings for the Mistral API
  message Mistral {
    // Auth Token to use for the Mistral API.
    // This token will be placed into the `Authorization` header
    // and prefixed with Bearer if not present
    // when sending the request to the upstream.
    SingleAuthToken auth_token = 1;
    // Optional custom host to send the traffic to
    CustomHost custom_host = 2;
  }

  message Anthropic {
    // Auth Token to use for the Anthropic API.
    // This token will be placed into the `x-api-key` header
    // when sending the request to the upstream.
    SingleAuthToken auth_token = 1;

    CustomHost custom_host = 2;
    // An optional version header to pass to the Anthropic API
    // See: https://docs.anthropic.com/en/api/versioning for more details
    string version = 3;
  }
  
  oneof llm {
    OpenAI openai = 1;
    Mistral mistral = 2;
    Anthropic anthropic = 3;
  }
}

/*
  RouteSettings is a way to configure the behavior of the LLM provider on a per-route basis
  This allows users to configure things like:
  - Prompt Enrichment
  - Retrieval Augmented Generation
  - Semantic Caching
  - Backup Models
  - Defaults to merge with the user input fields
  - Guardrails

  NOTE: These settings may only be applied to a route which uses an LLMProvider backend!
*/
message RouteSettings {
  // Config used to enrich the prompt. This can only be used with LLMProviders using the CHAT API type.
  AIPromptEnrichment prompt_enrichment = 1;

  // Guards to apply to the LLM requests on this route
  AIPromptGaurd prompt_guard = 2;

  // Rate limiting configuration to apply to the corresponding routes.
  // All Rate limiting applied this way will use the input_tokens as the counter
  // rather than incrementing by 1 for each request.
  RateLimiting rate_limiting = 3;

  // Retrieval Augmented Generation. https://research.ibm.com/blog/retrieval-augmented-generation-RAG
  // Retrieval Augmented Generation is a process by which you "augment" the information
  // a model has access to by providing it with a set of documents to use as context.
  // This can be used to improve the quality of the generated text.
  // Important Note: The same embedding mechanism must be used for the prompt
  // which was used for the initial creation of the context documents.
  RAG rag = 4;

  // Semantic caching configuration
  // Semantic caching allows you to cache previous model responses in order to provide 
  // faster responses to similar requests in the future.
  // Results will vary depending on the embedding mechanism used, as well
  // as the similarity threshold set.
  SemanticCache semantic_cache = 5;

  // Backup models to use in case of a failure with the primary model
  // passed in the request. By default each model will be tried 2 times
  // before moving on to the next model in the list. If all requests fail then 
  // the final response will be returned to the client.
  repeated string backup_models = 6;

  // A list of defaults to be merged with the user input fields.
  // These will NOT override the user input fields unless override is explicitly set to true.
  // Some examples include setting the temperature, max_tokens, etc.
  repeated FieldDefault defaults = 7;
}

message FieldDefault {
  // Field name
  string field = 1;
  // Field Value, this can be any valid JSON value
  google.protobuf.Value value = 2;
  // Whether or not to override the field if it already exists
  bool override = 3;
}


message Postgres {
  // Connection string to the Postgres database
  string connection_string = 1;
  // Name of the table to use
  string collection_name = 2;
}



message Embedding {
  message OpenAI {
    oneof auth_token_source {
      SingleAuthToken auth_token = 1;
      // re-use the token from the backend
      // google.protobuf.Empty inherit_backend_token = 3;
    }
  }

  oneof embedding {
    OpenAI openai = 1;
  }
}

message SemanticCache {
  message Redis {
    // Connection string to the Redis database
    string connection_string = 1;
  }
  // Data store from which to cache the request/response pairs
  message DataStore {
    oneof datastore {
      Redis redis = 1;
    }
  }
  // Which data store to use
  DataStore datastore = 1;
  // Model to use to get embeddings for prompt
  Embedding embedding = 2;
  // Time before data in the cache is considered expired
  uint32 ttl = 3;
}

message RAG {
  message DataStore {
    oneof datastore {
      Postgres postgres = 1;
    }
  }
  // Data store from which to fetch the embeddings
  DataStore datastore = 1;
  // Model to use to get embeddings for prompt
  Embedding embedding = 2;
  // Template to use to embed the returned context
  string prompt_template = 3;
}

message RateLimiting {
  // List of rate_limit configs to apply
  repeated string rate_limit_configs = 1;
}

message AIPromptEnrichment {
  message Message {
    // Role of the message.
    // The available roles depend on the backend model being used,
    // please consult the documentation for more information.
    string role = 1;
    // String content of the message
    string content = 2;
  }
  // A list of messages to be prepended to the prompt sent by the client
  repeated Message prepend = 2;
  // A list of messages to be appended to the prompt sent by the client
  repeated Message append = 3;

}

message AIPromptGaurd {
  message Request {
    enum BuiltIn {
      SSN = 0;
      CREDIT_CARD = 1;
      EMAIL = 2;
      PHONE_NUMBER = 3;
    }
    // All matches will be masked before being sent back to the client
    // We can also use DLP stats to see how often this happens
    repeated string matches = 1;
    // A list of built-in regexes to mask in the response
    repeated BuiltIn builtins = 2;
    // Custom response message to send back to the client
    string custom_response_message = 3;
  }

  message Response {
      enum BuiltIn {
        SSN = 0;
        CREDIT_CARD = 1;
        EMAIL = 2;
        PHONE_NUMBER = 3;
      }
      // All matches will be masked before being sent back to the client
      // We can also use DLP stats to see how often this happens
      repeated string matches = 1;
      // A list of built-in regexes to mask in the response
      repeated BuiltIn builtins = 2;
  }
  // Guards for the prompt request
  Request request = 2;
  // Guards for the LLM response
  Response response = 3;
}