// Code generated by skv2. DO NOT EDIT.

// Definition for federated resource reconciler templates.
package federation

import (
	"context"

	"github.com/hashicorp/go-multierror"
	"github.com/solo-io/go-utils/contextutils"
	"github.com/solo-io/go-utils/stringutils"
	"github.com/solo-io/skv2/pkg/reconcile"
	ratelimit_solo_io_v1alpha1 "github.com/solo-io/solo-apis/pkg/api/ratelimit.solo.io/v1alpha1"
	ratelimit_solo_io_v1alpha1_sets "github.com/solo-io/solo-apis/pkg/api/ratelimit.solo.io/v1alpha1/sets"
	fed_ratelimit_solo_io_v1alpha1 "github.com/solo-io/solo-projects/projects/gloo-fed/pkg/api/fed.ratelimit.solo.io/v1alpha1"
	"github.com/solo-io/solo-projects/projects/gloo-fed/pkg/api/fed.ratelimit.solo.io/v1alpha1/controller"
	mc_types "github.com/solo-io/solo-projects/projects/gloo-fed/pkg/api/fed.solo.io/core/v1"
	"github.com/solo-io/solo-projects/projects/gloo-fed/pkg/federation"
	"github.com/solo-io/solo-projects/projects/gloo-fed/pkg/federation/placement"
	"github.com/solo-io/solo-projects/projects/gloo-fed/pkg/multicluster"
	"go.uber.org/zap"
	"k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

type federatedRateLimitConfigReconciler struct {
	ctx                       context.Context
	federatedRateLimitConfigs fed_ratelimit_solo_io_v1alpha1.FederatedRateLimitConfigClient
	baseClients               ratelimit_solo_io_v1alpha1.MulticlusterClientset
	placementManager          placement.Manager
	clusterSet                multicluster.ClusterSet
}

func NewFederatedRateLimitConfigReconciler(
	ctx context.Context,
	federatedRateLimitConfigs fed_ratelimit_solo_io_v1alpha1.FederatedRateLimitConfigClient,
	baseClients ratelimit_solo_io_v1alpha1.MulticlusterClientset,
	placementManager placement.Manager,
	clusterSet multicluster.ClusterSet,
) controller.FederatedRateLimitConfigFinalizer {
	return &federatedRateLimitConfigReconciler{
		ctx:                       ctx,
		federatedRateLimitConfigs: federatedRateLimitConfigs,
		baseClients:               baseClients,
		placementManager:          placementManager,
		clusterSet:                clusterSet,
	}
}

func (f *federatedRateLimitConfigReconciler) ReconcileFederatedRateLimitConfig(obj *fed_ratelimit_solo_io_v1alpha1.FederatedRateLimitConfig) (reconcile.Result, error) {
	currentPlacementStatus := f.placementManager.GetPlacementStatus(&obj.Status)
	needsReconcile := obj.NeedsReconcile(currentPlacementStatus)
	allClusters := f.clusterSet.ListClusters()
	contextutils.LoggerFrom(f.ctx).Debugw("ReconcileFederatedRateLimitConfig", zap.Any("FederatedRateLimitConfig", obj), zap.Any("needsReconcile", needsReconcile),
		zap.Any("allClusters", allClusters))

	if !needsReconcile {
		return reconcile.Result{}, nil
	}

	statusBuilder := f.placementManager.GetBuilder()

	// Validate resource
	if obj.Spec.GetPlacement() == nil {
		updatedPlacementStatus := statusBuilder.
			UpdateUnprocessed(currentPlacementStatus, placement.PlacementMissing, mc_types.PlacementStatus_INVALID).
			Eject(obj.GetGeneration())
		f.placementManager.SetPlacementStatus(&obj.Status, updatedPlacementStatus)
		return reconcile.Result{}, f.federatedRateLimitConfigs.UpdateFederatedRateLimitConfigStatus(f.ctx, obj)
	}
	for _, cluster := range obj.Spec.Placement.GetClusters() {
		if !stringutils.ContainsString(cluster, allClusters) {
			updatedPlacementStatus := statusBuilder.
				UpdateUnprocessed(currentPlacementStatus, placement.ClusterNotRegistered(cluster), mc_types.PlacementStatus_INVALID).
				Eject(obj.GetGeneration())
			f.placementManager.SetPlacementStatus(&obj.Status, updatedPlacementStatus)
			return reconcile.Result{}, f.federatedRateLimitConfigs.UpdateFederatedRateLimitConfigStatus(f.ctx, obj)
		}
	}
	if obj.Spec.Template.GetSpec() == nil {
		updatedPlacementStatus := statusBuilder.
			UpdateUnprocessed(currentPlacementStatus, placement.SpecTemplateMissing, mc_types.PlacementStatus_INVALID).
			Eject(obj.GetGeneration())
		f.placementManager.SetPlacementStatus(&obj.Status, updatedPlacementStatus)
		return reconcile.Result{}, f.federatedRateLimitConfigs.UpdateFederatedRateLimitConfigStatus(f.ctx, obj)
	}
	if obj.Spec.Template.GetMetadata() == nil {
		updatedPlacementStatus := statusBuilder.
			UpdateUnprocessed(currentPlacementStatus, placement.MetaTemplateMissing, mc_types.PlacementStatus_INVALID).
			Eject(obj.GetGeneration())
		f.placementManager.SetPlacementStatus(&obj.Status, updatedPlacementStatus)
		return reconcile.Result{}, f.federatedRateLimitConfigs.UpdateFederatedRateLimitConfigStatus(f.ctx, obj)
	}

	// ownerLabel is used to reference Federated resources via their children.
	ownerLabel := federation.GetOwnerLabel(obj)
	// since ownerLabel may be truncated (max length 63), we also store the full value in an annotation
	// for verification in case of collisions
	ownerAnnotation := federation.GetOwnerAnnotation(obj)

	spec := obj.Spec.Template.GetSpec()
	meta := obj.Spec.Template.GetMetadata()
	labels := federation.Merge(meta.GetLabels(), ownerLabel)
	annotations := federation.Merge(meta.GetAnnotations(), ownerAnnotation)

	multiErr := &multierror.Error{}
	for _, cluster := range allClusters {
		clusterRateLimitConfigs := ratelimit_solo_io_v1alpha1_sets.NewRateLimitConfigSet()
		if stringutils.ContainsString(cluster, obj.Spec.Placement.GetClusters()) {
			for _, namespace := range obj.Spec.Placement.GetNamespaces() {

				clusterRateLimitConfigs.Insert(&ratelimit_solo_io_v1alpha1.RateLimitConfig{
					ObjectMeta: metav1.ObjectMeta{
						Namespace:   namespace,
						Name:        meta.GetName(),
						Labels:      labels,
						Annotations: annotations,
					},
					Spec: *spec,
				})
			}
		}

		if err := f.ensureCluster(cluster, statusBuilder, clusterRateLimitConfigs, obj); err != nil {
			multiErr.Errors = append(multiErr.Errors, err)
		}
	}

	f.placementManager.SetPlacementStatus(&obj.Status, statusBuilder.Build(obj.GetGeneration()))
	err := f.federatedRateLimitConfigs.UpdateFederatedRateLimitConfigStatus(f.ctx, obj)
	if err != nil {
		multiErr.Errors = append(multiErr.Errors, err)
		contextutils.LoggerFrom(f.ctx).Errorw("Failed to update status on federated rateLimitConfig", zap.Error(err))
	}

	return reconcile.Result{}, multiErr.ErrorOrNil()
}

func (f *federatedRateLimitConfigReconciler) FederatedRateLimitConfigFinalizerName() string {
	return federation.HubFinalizer
}

func (f *federatedRateLimitConfigReconciler) FinalizeFederatedRateLimitConfig(obj *fed_ratelimit_solo_io_v1alpha1.FederatedRateLimitConfig) error {
	return f.deleteAll(obj)
}

// ensureCluster upserts all desired resources on the given cluster.
// An error is returned only if a retry is expected to resolve the issue.
func (f *federatedRateLimitConfigReconciler) ensureCluster(cluster string, statusBuilder placement.StatusBuilder, desired ratelimit_solo_io_v1alpha1_sets.RateLimitConfigSet, fedResource *fed_ratelimit_solo_io_v1alpha1.FederatedRateLimitConfig) error {
	clientset, err := f.baseClients.Cluster(cluster)
	if err != nil {
		var namespaces []string
		for _, obj := range desired.List() {
			namespaces = append(namespaces, obj.GetNamespace())
		}

		contextutils.LoggerFrom(f.ctx).Errorw("Failed to get clientset", zap.String("cluster", cluster), zap.Error(err))
		statusBuilder.AddDestinations([]string{cluster}, namespaces, mc_types.PlacementStatus_Namespace{
			State:   mc_types.PlacementStatus_FAILED,
			Message: placement.FailedToCreateClientForCluster(cluster),
		})
		return err
	}

	rateLimitConfigClient := clientset.RateLimitConfigs()

	ownerLabel := federation.GetOwnerLabel(fedResource)
	existingList, err := rateLimitConfigClient.ListRateLimitConfig(f.ctx, client.MatchingLabels(ownerLabel))
	if err != nil {
		var namespaces []string
		for _, obj := range desired.List() {
			namespaces = append(namespaces, obj.GetNamespace())
		}

		contextutils.LoggerFrom(f.ctx).Errorw("Failed to list RateLimitConfigs", zap.String("cluster", cluster), zap.Error(err))
		statusBuilder.AddDestinations([]string{cluster}, namespaces, mc_types.PlacementStatus_Namespace{
			State:   mc_types.PlacementStatus_FAILED,
			Message: placement.FailedToListResource("rateLimitConfig", cluster),
		})
		return err
	}

	existing := ratelimit_solo_io_v1alpha1_sets.NewRateLimitConfigSet()
	for _, rateLimitConfig := range existingList.Items {
		// double-check that this RateLimitConfig is actually owned by this FederatedRateLimitConfig
		if !f.verifyOwner(fedResource, rateLimitConfig) {
			continue
		}
		rateLimitConfigPointer := rateLimitConfig
		existing.Insert(&rateLimitConfigPointer)
	}

	multiErr := &multierror.Error{}
	for _, desiredRateLimitConfig := range desired.List() {
		err := rateLimitConfigClient.UpsertRateLimitConfig(f.ctx, desiredRateLimitConfig)
		if err != nil && errors.IsConflict(err) {
			multiErr.Errors = append(multiErr.Errors, err)
			contextutils.LoggerFrom(f.ctx).Errorw("Failed to upsert rateLimitConfig due to resource conflict", zap.Error(err))
			statusBuilder.AddDestination(cluster, desiredRateLimitConfig.Namespace, mc_types.PlacementStatus_Namespace{
				State:   mc_types.PlacementStatus_FAILED,
				Message: placement.FailedToUpsertResourceDueToConflict("rateLimitConfig"),
			})
		} else if err != nil {
			multiErr.Errors = append(multiErr.Errors, err)
			contextutils.LoggerFrom(f.ctx).Errorw("Failed to upsert rateLimitConfig", zap.Error(err))
			statusBuilder.AddDestination(cluster, desiredRateLimitConfig.Namespace, mc_types.PlacementStatus_Namespace{
				State:   mc_types.PlacementStatus_FAILED,
				Message: placement.FailedToUpsertResource("rateLimitConfig"),
			})
		} else {
			statusBuilder.AddDestination(cluster, desiredRateLimitConfig.Namespace, mc_types.PlacementStatus_Namespace{
				State: mc_types.PlacementStatus_PLACED,
			})
		}
	}

	for _, staleRateLimitConfig := range existing.Difference(desired).List() {
		err := rateLimitConfigClient.DeleteRateLimitConfig(f.ctx, client.ObjectKey{
			Namespace: staleRateLimitConfig.Namespace,
			Name:      staleRateLimitConfig.Name,
		})
		if client.IgnoreNotFound(err) != nil {
			contextutils.LoggerFrom(f.ctx).Errorw("Failed to delete rateLimitConfig", zap.Error(err))
			statusBuilder.AddDestination(cluster, staleRateLimitConfig.Namespace, mc_types.PlacementStatus_Namespace{
				State:   mc_types.PlacementStatus_STALE,
				Message: placement.FailedToDeleteResource("rateLimitConfig"),
			})
		}
	}

	return multiErr.ErrorOrNil()
}

// Delete all RateLimitConfigs managed by the given FederatedRateLimitConfig on all clusters.
// Used to ensure that RateLimitConfigs generated by a FederatedRateLimitConfig are cleaned up on delete.
func (f *federatedRateLimitConfigReconciler) deleteAll(fedResource *fed_ratelimit_solo_io_v1alpha1.FederatedRateLimitConfig) error {
	ownerLabel := federation.GetOwnerLabel(fedResource)
	for _, cluster := range f.clusterSet.ListClusters() {
		clusterClient, err := f.baseClients.Cluster(cluster)
		if err != nil {
			return err
		}
		// TODO this requires permissions in all namespaces, we could restrict to to namespaces referenced by gloo instances
		list, err := clusterClient.RateLimitConfigs().ListRateLimitConfig(f.ctx, client.MatchingLabels(ownerLabel))
		if err != nil {
			return err
		}

		for _, rateLimitConfig := range list.Items {
			// double-check that this RateLimitConfig is actually owned by this FederatedRateLimitConfig
			if !f.verifyOwner(fedResource, rateLimitConfig) {
				continue
			}
			err = clusterClient.RateLimitConfigs().DeleteRateLimitConfig(f.ctx, client.ObjectKey{
				Namespace: rateLimitConfig.Namespace,
				Name:      rateLimitConfig.Name,
			})
			if client.IgnoreNotFound(err) != nil {
				return err
			}
		}
	}
	return nil
}

// Ensures that the given federated resource is the owner of the given resource, by checking that the owner annotation on the resource matches the federated resource.
func (f *federatedRateLimitConfigReconciler) verifyOwner(fedResource *fed_ratelimit_solo_io_v1alpha1.FederatedRateLimitConfig, resource ratelimit_solo_io_v1alpha1.RateLimitConfig) bool {
	fedResourceId := federation.GetIdentifier(fedResource)
	// Get the value of the owner annotation on the given resource.
	resourceOwner := resource.GetAnnotations()[federation.HubOwner]
	// This value may be empty (e.g. in earlier versions of Gloo Fed before we added this annotation), so in that case return true.
	// This is safe because before we added the annotation, we always persisted the entire identifier (no truncation) so as long as the label matches,
	// then it's a verified owner match.
	if resourceOwner == "" {
		return true
	}
	return resourceOwner == fedResourceId
}
