// Code generated by skv2. DO NOT EDIT.

// Definition for federated resource cluster handler templates
package federation

import (
	"context"

	"github.com/avast/retry-go/v4"
	"github.com/solo-io/go-utils/contextutils"
	"github.com/solo-io/skv2/pkg/multicluster"
	fed_ratelimit_solo_io_v1alpha1 "github.com/solo-io/solo-projects/projects/gloo-fed/pkg/api/fed.ratelimit.solo.io/v1alpha1"
	mc_types "github.com/solo-io/solo-projects/projects/gloo-fed/pkg/api/fed.solo.io/core/v1"
	"github.com/solo-io/solo-projects/projects/gloo-fed/pkg/federation"
	"github.com/solo-io/solo-projects/projects/gloo-fed/pkg/federation/placement"
	"go.uber.org/zap"
	"k8s.io/apimachinery/pkg/api/errors"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/manager"
)

type clusterHandler struct {
	ctx     context.Context
	clients fed_ratelimit_solo_io_v1alpha1.Clientset
	manager placement.Manager
}

func NewClusterHandler(ctx context.Context, clients fed_ratelimit_solo_io_v1alpha1.Clientset, manager placement.Manager) multicluster.ClusterHandler {
	return &clusterHandler{
		ctx:     ctx,
		clients: clients,
		manager: manager,
	}
}

func (f *clusterHandler) AddCluster(_ context.Context, cluster string, _ manager.Manager) {
	f.handleClusterEvent(cluster)
}

func (f *clusterHandler) RemoveCluster(cluster string) {
	f.handleClusterEvent(cluster)
}

func (f *clusterHandler) handleClusterEvent(cluster string) {

	federatedRateLimitConfigList, err := f.clients.FederatedRateLimitConfigs().ListFederatedRateLimitConfig(f.ctx)
	if err != nil {
		contextutils.LoggerFrom(f.ctx).Errorf("Failed to list FederatedRateLimitConfigs referencing cluster %s", cluster)
	} else {
		for _, item := range federatedRateLimitConfigList.Items {
			item := item
			if err := f.maybeUpdateFederatedRateLimitConfigStatusWithRetries(&item, cluster); err != nil {
				contextutils.LoggerFrom(f.ctx).Errorw("Failed to update status on FederatedRateLimitConfig",
					zap.Error(err),
					zap.Any("FederatedRateLimitConfig", item))
			}
		}
	}
}

func (f *clusterHandler) maybeUpdateFederatedRateLimitConfigStatusWithRetries(item *fed_ratelimit_solo_io_v1alpha1.FederatedRateLimitConfig, cluster string) error {
	return retry.Do(func() error {
		err := f.maybeUpdateFederatedRateLimitConfigStatus(item, cluster)
		if err != nil && errors.IsNotFound(err) {
			// If the resource no longer exists, there is nothing to do.
			return nil
		} else if err != nil {
			// On conflict, retry with the new object to pick up any changes to the resource's spec.
			obj, err := f.clients.FederatedRateLimitConfigs().GetFederatedRateLimitConfig(f.ctx, client.ObjectKey{Namespace: item.Namespace, Name: item.Name})
			if err != nil {
				return err
			}
			item = obj
		}
		return err
	}, federation.GetClusterWatcherLocalRetryOptions(f.ctx)...)
}

func (f *clusterHandler) maybeUpdateFederatedRateLimitConfigStatus(item *fed_ratelimit_solo_io_v1alpha1.FederatedRateLimitConfig, cluster string) error {
	for _, c := range item.Spec.Placement.GetClusters() {
		if c == cluster {
			currentPlacementStatus := f.manager.GetPlacementStatus(&item.Status)

			// An existing resource references the given cluster. Update its status to trigger a resync.
			updatedPlacementStatus := f.manager.GetBuilder().
				UpdateUnprocessed(currentPlacementStatus, placement.ClusterEventTriggered(cluster), mc_types.PlacementStatus_PENDING).
				// Do not update the observed generation or written by fields as we have not actually processed the resource.
				Eject(currentPlacementStatus.GetObservedGeneration())
			f.manager.SetPlacementStatus(&item.Status, updatedPlacementStatus)

			return f.clients.FederatedRateLimitConfigs().UpdateFederatedRateLimitConfigStatus(f.ctx, item)
		}
	}
	return nil
}
