// Code generated by skv2. DO NOT EDIT.

// Definition for federated resource reconciler templates.
package federation

import (
	"context"

	"github.com/hashicorp/go-multierror"
	"github.com/solo-io/go-utils/contextutils"
	"github.com/solo-io/go-utils/stringutils"
	"github.com/solo-io/skv2/pkg/reconcile"
	gloo_solo_io_v1 "github.com/solo-io/solo-apis/pkg/api/gloo.solo.io/v1"
	gloo_solo_io_v1_sets "github.com/solo-io/solo-apis/pkg/api/gloo.solo.io/v1/sets"
	fed_gloo_solo_io_v1 "github.com/solo-io/solo-projects/projects/gloo-fed/pkg/api/fed.gloo.solo.io/v1"
	"github.com/solo-io/solo-projects/projects/gloo-fed/pkg/api/fed.gloo.solo.io/v1/controller"
	mc_types "github.com/solo-io/solo-projects/projects/gloo-fed/pkg/api/fed.solo.io/core/v1"
	"github.com/solo-io/solo-projects/projects/gloo-fed/pkg/federation"
	"github.com/solo-io/solo-projects/projects/gloo-fed/pkg/federation/placement"
	"github.com/solo-io/solo-projects/projects/gloo-fed/pkg/multicluster"
	"go.uber.org/zap"
	"k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

type federatedUpstreamReconciler struct {
	ctx                context.Context
	federatedUpstreams fed_gloo_solo_io_v1.FederatedUpstreamClient
	baseClients        gloo_solo_io_v1.MulticlusterClientset
	placementManager   placement.Manager
	clusterSet         multicluster.ClusterSet
}

func NewFederatedUpstreamReconciler(
	ctx context.Context,
	federatedUpstreams fed_gloo_solo_io_v1.FederatedUpstreamClient,
	baseClients gloo_solo_io_v1.MulticlusterClientset,
	placementManager placement.Manager,
	clusterSet multicluster.ClusterSet,
) controller.FederatedUpstreamFinalizer {
	return &federatedUpstreamReconciler{
		ctx:                ctx,
		federatedUpstreams: federatedUpstreams,
		baseClients:        baseClients,
		placementManager:   placementManager,
		clusterSet:         clusterSet,
	}
}

func (f *federatedUpstreamReconciler) ReconcileFederatedUpstream(obj *fed_gloo_solo_io_v1.FederatedUpstream) (reconcile.Result, error) {
	currentPlacementStatus := f.placementManager.GetPlacementStatus(&obj.Status)
	needsReconcile := obj.NeedsReconcile(currentPlacementStatus)
	allClusters := f.clusterSet.ListClusters()
	contextutils.LoggerFrom(f.ctx).Debugw("ReconcileFederatedUpstream", zap.Any("FederatedUpstream", obj), zap.Any("needsReconcile", needsReconcile),
		zap.Any("allClusters", allClusters))

	if !needsReconcile {
		return reconcile.Result{}, nil
	}

	statusBuilder := f.placementManager.GetBuilder()

	// Validate resource
	if obj.Spec.GetPlacement() == nil {
		updatedPlacementStatus := statusBuilder.
			UpdateUnprocessed(currentPlacementStatus, placement.PlacementMissing, mc_types.PlacementStatus_INVALID).
			Eject(obj.GetGeneration())
		f.placementManager.SetPlacementStatus(&obj.Status, updatedPlacementStatus)
		return reconcile.Result{}, f.federatedUpstreams.UpdateFederatedUpstreamStatus(f.ctx, obj)
	}
	for _, cluster := range obj.Spec.Placement.GetClusters() {
		if !stringutils.ContainsString(cluster, allClusters) {
			updatedPlacementStatus := statusBuilder.
				UpdateUnprocessed(currentPlacementStatus, placement.ClusterNotRegistered(cluster), mc_types.PlacementStatus_INVALID).
				Eject(obj.GetGeneration())
			f.placementManager.SetPlacementStatus(&obj.Status, updatedPlacementStatus)
			return reconcile.Result{}, f.federatedUpstreams.UpdateFederatedUpstreamStatus(f.ctx, obj)
		}
	}
	if obj.Spec.Template.GetSpec() == nil {
		updatedPlacementStatus := statusBuilder.
			UpdateUnprocessed(currentPlacementStatus, placement.SpecTemplateMissing, mc_types.PlacementStatus_INVALID).
			Eject(obj.GetGeneration())
		f.placementManager.SetPlacementStatus(&obj.Status, updatedPlacementStatus)
		return reconcile.Result{}, f.federatedUpstreams.UpdateFederatedUpstreamStatus(f.ctx, obj)
	}
	if obj.Spec.Template.GetMetadata() == nil {
		updatedPlacementStatus := statusBuilder.
			UpdateUnprocessed(currentPlacementStatus, placement.MetaTemplateMissing, mc_types.PlacementStatus_INVALID).
			Eject(obj.GetGeneration())
		f.placementManager.SetPlacementStatus(&obj.Status, updatedPlacementStatus)
		return reconcile.Result{}, f.federatedUpstreams.UpdateFederatedUpstreamStatus(f.ctx, obj)
	}

	// ownerLabel is used to reference Federated resources via their children.
	ownerLabel := federation.GetOwnerLabel(obj)
	// since ownerLabel may be truncated (max length 63), we also store the full value in an annotation
	// for verification in case of collisions
	ownerAnnotation := federation.GetOwnerAnnotation(obj)

	spec := obj.Spec.Template.GetSpec()
	meta := obj.Spec.Template.GetMetadata()
	labels := federation.Merge(meta.GetLabels(), ownerLabel)
	annotations := federation.Merge(meta.GetAnnotations(), ownerAnnotation)

	multiErr := &multierror.Error{}
	for _, cluster := range allClusters {
		clusterUpstreams := gloo_solo_io_v1_sets.NewUpstreamSet()
		if stringutils.ContainsString(cluster, obj.Spec.Placement.GetClusters()) {
			for _, namespace := range obj.Spec.Placement.GetNamespaces() {

				clusterUpstreams.Insert(&gloo_solo_io_v1.Upstream{
					ObjectMeta: metav1.ObjectMeta{
						Namespace:   namespace,
						Name:        meta.GetName(),
						Labels:      labels,
						Annotations: annotations,
					},
					Spec: *spec,
				})
			}
		}

		if err := f.ensureCluster(cluster, statusBuilder, clusterUpstreams, obj); err != nil {
			multiErr.Errors = append(multiErr.Errors, err)
		}
	}

	f.placementManager.SetPlacementStatus(&obj.Status, statusBuilder.Build(obj.GetGeneration()))
	err := f.federatedUpstreams.UpdateFederatedUpstreamStatus(f.ctx, obj)
	if err != nil {
		multiErr.Errors = append(multiErr.Errors, err)
		contextutils.LoggerFrom(f.ctx).Errorw("Failed to update status on federated upstream", zap.Error(err))
	}

	return reconcile.Result{}, multiErr.ErrorOrNil()
}

func (f *federatedUpstreamReconciler) FederatedUpstreamFinalizerName() string {
	return federation.HubFinalizer
}

func (f *federatedUpstreamReconciler) FinalizeFederatedUpstream(obj *fed_gloo_solo_io_v1.FederatedUpstream) error {
	return f.deleteAll(obj)
}

// ensureCluster upserts all desired resources on the given cluster.
// An error is returned only if a retry is expected to resolve the issue.
func (f *federatedUpstreamReconciler) ensureCluster(cluster string, statusBuilder placement.StatusBuilder, desired gloo_solo_io_v1_sets.UpstreamSet, fedResource *fed_gloo_solo_io_v1.FederatedUpstream) error {
	clientset, err := f.baseClients.Cluster(cluster)
	if err != nil {
		var namespaces []string
		for _, obj := range desired.List() {
			namespaces = append(namespaces, obj.GetNamespace())
		}

		contextutils.LoggerFrom(f.ctx).Errorw("Failed to get clientset", zap.String("cluster", cluster), zap.Error(err))
		statusBuilder.AddDestinations([]string{cluster}, namespaces, mc_types.PlacementStatus_Namespace{
			State:   mc_types.PlacementStatus_FAILED,
			Message: placement.FailedToCreateClientForCluster(cluster),
		})
		return err
	}

	upstreamClient := clientset.Upstreams()

	ownerLabel := federation.GetOwnerLabel(fedResource)
	existingList, err := upstreamClient.ListUpstream(f.ctx, client.MatchingLabels(ownerLabel))
	if err != nil {
		var namespaces []string
		for _, obj := range desired.List() {
			namespaces = append(namespaces, obj.GetNamespace())
		}

		contextutils.LoggerFrom(f.ctx).Errorw("Failed to list Upstreams", zap.String("cluster", cluster), zap.Error(err))
		statusBuilder.AddDestinations([]string{cluster}, namespaces, mc_types.PlacementStatus_Namespace{
			State:   mc_types.PlacementStatus_FAILED,
			Message: placement.FailedToListResource("upstream", cluster),
		})
		return err
	}

	existing := gloo_solo_io_v1_sets.NewUpstreamSet()
	for _, upstream := range existingList.Items {
		// double-check that this Upstream is actually owned by this FederatedUpstream
		if !f.verifyOwner(fedResource, upstream) {
			continue
		}
		upstreamPointer := upstream
		existing.Insert(&upstreamPointer)
	}

	multiErr := &multierror.Error{}
	for _, desiredUpstream := range desired.List() {
		err := upstreamClient.UpsertUpstream(f.ctx, desiredUpstream)
		if err != nil && errors.IsConflict(err) {
			multiErr.Errors = append(multiErr.Errors, err)
			contextutils.LoggerFrom(f.ctx).Errorw("Failed to upsert upstream due to resource conflict", zap.Error(err))
			statusBuilder.AddDestination(cluster, desiredUpstream.Namespace, mc_types.PlacementStatus_Namespace{
				State:   mc_types.PlacementStatus_FAILED,
				Message: placement.FailedToUpsertResourceDueToConflict("upstream"),
			})
		} else if err != nil {
			multiErr.Errors = append(multiErr.Errors, err)
			contextutils.LoggerFrom(f.ctx).Errorw("Failed to upsert upstream", zap.Error(err))
			statusBuilder.AddDestination(cluster, desiredUpstream.Namespace, mc_types.PlacementStatus_Namespace{
				State:   mc_types.PlacementStatus_FAILED,
				Message: placement.FailedToUpsertResource("upstream"),
			})
		} else {
			statusBuilder.AddDestination(cluster, desiredUpstream.Namespace, mc_types.PlacementStatus_Namespace{
				State: mc_types.PlacementStatus_PLACED,
			})
		}
	}

	for _, staleUpstream := range existing.Difference(desired).List() {
		err := upstreamClient.DeleteUpstream(f.ctx, client.ObjectKey{
			Namespace: staleUpstream.Namespace,
			Name:      staleUpstream.Name,
		})
		if client.IgnoreNotFound(err) != nil {
			contextutils.LoggerFrom(f.ctx).Errorw("Failed to delete upstream", zap.Error(err))
			statusBuilder.AddDestination(cluster, staleUpstream.Namespace, mc_types.PlacementStatus_Namespace{
				State:   mc_types.PlacementStatus_STALE,
				Message: placement.FailedToDeleteResource("upstream"),
			})
		}
	}

	return multiErr.ErrorOrNil()
}

// Delete all Upstreams managed by the given FederatedUpstream on all clusters.
// Used to ensure that Upstreams generated by a FederatedUpstream are cleaned up on delete.
func (f *federatedUpstreamReconciler) deleteAll(fedResource *fed_gloo_solo_io_v1.FederatedUpstream) error {
	ownerLabel := federation.GetOwnerLabel(fedResource)
	for _, cluster := range f.clusterSet.ListClusters() {
		clusterClient, err := f.baseClients.Cluster(cluster)
		if err != nil {
			return err
		}
		// TODO this requires permissions in all namespaces, we could restrict to to namespaces referenced by gloo instances
		list, err := clusterClient.Upstreams().ListUpstream(f.ctx, client.MatchingLabels(ownerLabel))
		if err != nil {
			return err
		}

		for _, upstream := range list.Items {
			// double-check that this Upstream is actually owned by this FederatedUpstream
			if !f.verifyOwner(fedResource, upstream) {
				continue
			}
			err = clusterClient.Upstreams().DeleteUpstream(f.ctx, client.ObjectKey{
				Namespace: upstream.Namespace,
				Name:      upstream.Name,
			})
			if client.IgnoreNotFound(err) != nil {
				return err
			}
		}
	}
	return nil
}

// Ensures that the given federated resource is the owner of the given resource, by checking that the owner annotation on the resource matches the federated resource.
func (f *federatedUpstreamReconciler) verifyOwner(fedResource *fed_gloo_solo_io_v1.FederatedUpstream, resource gloo_solo_io_v1.Upstream) bool {
	fedResourceId := federation.GetIdentifier(fedResource)
	// Get the value of the owner annotation on the given resource.
	resourceOwner := resource.GetAnnotations()[federation.HubOwner]
	// This value may be empty (e.g. in earlier versions of Gloo Fed before we added this annotation), so in that case return true.
	// This is safe because before we added the annotation, we always persisted the entire identifier (no truncation) so as long as the label matches,
	// then it's a verified owner match.
	if resourceOwner == "" {
		return true
	}
	return resourceOwner == fedResourceId
}

type federatedUpstreamGroupReconciler struct {
	ctx                     context.Context
	federatedUpstreamGroups fed_gloo_solo_io_v1.FederatedUpstreamGroupClient
	baseClients             gloo_solo_io_v1.MulticlusterClientset
	placementManager        placement.Manager
	clusterSet              multicluster.ClusterSet
}

func NewFederatedUpstreamGroupReconciler(
	ctx context.Context,
	federatedUpstreamGroups fed_gloo_solo_io_v1.FederatedUpstreamGroupClient,
	baseClients gloo_solo_io_v1.MulticlusterClientset,
	placementManager placement.Manager,
	clusterSet multicluster.ClusterSet,
) controller.FederatedUpstreamGroupFinalizer {
	return &federatedUpstreamGroupReconciler{
		ctx:                     ctx,
		federatedUpstreamGroups: federatedUpstreamGroups,
		baseClients:             baseClients,
		placementManager:        placementManager,
		clusterSet:              clusterSet,
	}
}

func (f *federatedUpstreamGroupReconciler) ReconcileFederatedUpstreamGroup(obj *fed_gloo_solo_io_v1.FederatedUpstreamGroup) (reconcile.Result, error) {
	currentPlacementStatus := f.placementManager.GetPlacementStatus(&obj.Status)
	needsReconcile := obj.NeedsReconcile(currentPlacementStatus)
	allClusters := f.clusterSet.ListClusters()
	contextutils.LoggerFrom(f.ctx).Debugw("ReconcileFederatedUpstreamGroup", zap.Any("FederatedUpstreamGroup", obj), zap.Any("needsReconcile", needsReconcile),
		zap.Any("allClusters", allClusters))

	if !needsReconcile {
		return reconcile.Result{}, nil
	}

	statusBuilder := f.placementManager.GetBuilder()

	// Validate resource
	if obj.Spec.GetPlacement() == nil {
		updatedPlacementStatus := statusBuilder.
			UpdateUnprocessed(currentPlacementStatus, placement.PlacementMissing, mc_types.PlacementStatus_INVALID).
			Eject(obj.GetGeneration())
		f.placementManager.SetPlacementStatus(&obj.Status, updatedPlacementStatus)
		return reconcile.Result{}, f.federatedUpstreamGroups.UpdateFederatedUpstreamGroupStatus(f.ctx, obj)
	}
	for _, cluster := range obj.Spec.Placement.GetClusters() {
		if !stringutils.ContainsString(cluster, allClusters) {
			updatedPlacementStatus := statusBuilder.
				UpdateUnprocessed(currentPlacementStatus, placement.ClusterNotRegistered(cluster), mc_types.PlacementStatus_INVALID).
				Eject(obj.GetGeneration())
			f.placementManager.SetPlacementStatus(&obj.Status, updatedPlacementStatus)
			return reconcile.Result{}, f.federatedUpstreamGroups.UpdateFederatedUpstreamGroupStatus(f.ctx, obj)
		}
	}
	if obj.Spec.Template.GetSpec() == nil {
		updatedPlacementStatus := statusBuilder.
			UpdateUnprocessed(currentPlacementStatus, placement.SpecTemplateMissing, mc_types.PlacementStatus_INVALID).
			Eject(obj.GetGeneration())
		f.placementManager.SetPlacementStatus(&obj.Status, updatedPlacementStatus)
		return reconcile.Result{}, f.federatedUpstreamGroups.UpdateFederatedUpstreamGroupStatus(f.ctx, obj)
	}
	if obj.Spec.Template.GetMetadata() == nil {
		updatedPlacementStatus := statusBuilder.
			UpdateUnprocessed(currentPlacementStatus, placement.MetaTemplateMissing, mc_types.PlacementStatus_INVALID).
			Eject(obj.GetGeneration())
		f.placementManager.SetPlacementStatus(&obj.Status, updatedPlacementStatus)
		return reconcile.Result{}, f.federatedUpstreamGroups.UpdateFederatedUpstreamGroupStatus(f.ctx, obj)
	}

	// ownerLabel is used to reference Federated resources via their children.
	ownerLabel := federation.GetOwnerLabel(obj)
	// since ownerLabel may be truncated (max length 63), we also store the full value in an annotation
	// for verification in case of collisions
	ownerAnnotation := federation.GetOwnerAnnotation(obj)

	spec := obj.Spec.Template.GetSpec()
	meta := obj.Spec.Template.GetMetadata()
	labels := federation.Merge(meta.GetLabels(), ownerLabel)
	annotations := federation.Merge(meta.GetAnnotations(), ownerAnnotation)

	multiErr := &multierror.Error{}
	for _, cluster := range allClusters {
		clusterUpstreamGroups := gloo_solo_io_v1_sets.NewUpstreamGroupSet()
		if stringutils.ContainsString(cluster, obj.Spec.Placement.GetClusters()) {
			for _, namespace := range obj.Spec.Placement.GetNamespaces() {

				clusterUpstreamGroups.Insert(&gloo_solo_io_v1.UpstreamGroup{
					ObjectMeta: metav1.ObjectMeta{
						Namespace:   namespace,
						Name:        meta.GetName(),
						Labels:      labels,
						Annotations: annotations,
					},
					Spec: *spec,
				})
			}
		}

		if err := f.ensureCluster(cluster, statusBuilder, clusterUpstreamGroups, obj); err != nil {
			multiErr.Errors = append(multiErr.Errors, err)
		}
	}

	f.placementManager.SetPlacementStatus(&obj.Status, statusBuilder.Build(obj.GetGeneration()))
	err := f.federatedUpstreamGroups.UpdateFederatedUpstreamGroupStatus(f.ctx, obj)
	if err != nil {
		multiErr.Errors = append(multiErr.Errors, err)
		contextutils.LoggerFrom(f.ctx).Errorw("Failed to update status on federated upstreamGroup", zap.Error(err))
	}

	return reconcile.Result{}, multiErr.ErrorOrNil()
}

func (f *federatedUpstreamGroupReconciler) FederatedUpstreamGroupFinalizerName() string {
	return federation.HubFinalizer
}

func (f *federatedUpstreamGroupReconciler) FinalizeFederatedUpstreamGroup(obj *fed_gloo_solo_io_v1.FederatedUpstreamGroup) error {
	return f.deleteAll(obj)
}

// ensureCluster upserts all desired resources on the given cluster.
// An error is returned only if a retry is expected to resolve the issue.
func (f *federatedUpstreamGroupReconciler) ensureCluster(cluster string, statusBuilder placement.StatusBuilder, desired gloo_solo_io_v1_sets.UpstreamGroupSet, fedResource *fed_gloo_solo_io_v1.FederatedUpstreamGroup) error {
	clientset, err := f.baseClients.Cluster(cluster)
	if err != nil {
		var namespaces []string
		for _, obj := range desired.List() {
			namespaces = append(namespaces, obj.GetNamespace())
		}

		contextutils.LoggerFrom(f.ctx).Errorw("Failed to get clientset", zap.String("cluster", cluster), zap.Error(err))
		statusBuilder.AddDestinations([]string{cluster}, namespaces, mc_types.PlacementStatus_Namespace{
			State:   mc_types.PlacementStatus_FAILED,
			Message: placement.FailedToCreateClientForCluster(cluster),
		})
		return err
	}

	upstreamGroupClient := clientset.UpstreamGroups()

	ownerLabel := federation.GetOwnerLabel(fedResource)
	existingList, err := upstreamGroupClient.ListUpstreamGroup(f.ctx, client.MatchingLabels(ownerLabel))
	if err != nil {
		var namespaces []string
		for _, obj := range desired.List() {
			namespaces = append(namespaces, obj.GetNamespace())
		}

		contextutils.LoggerFrom(f.ctx).Errorw("Failed to list UpstreamGroups", zap.String("cluster", cluster), zap.Error(err))
		statusBuilder.AddDestinations([]string{cluster}, namespaces, mc_types.PlacementStatus_Namespace{
			State:   mc_types.PlacementStatus_FAILED,
			Message: placement.FailedToListResource("upstreamGroup", cluster),
		})
		return err
	}

	existing := gloo_solo_io_v1_sets.NewUpstreamGroupSet()
	for _, upstreamGroup := range existingList.Items {
		// double-check that this UpstreamGroup is actually owned by this FederatedUpstreamGroup
		if !f.verifyOwner(fedResource, upstreamGroup) {
			continue
		}
		upstreamGroupPointer := upstreamGroup
		existing.Insert(&upstreamGroupPointer)
	}

	multiErr := &multierror.Error{}
	for _, desiredUpstreamGroup := range desired.List() {
		err := upstreamGroupClient.UpsertUpstreamGroup(f.ctx, desiredUpstreamGroup)
		if err != nil && errors.IsConflict(err) {
			multiErr.Errors = append(multiErr.Errors, err)
			contextutils.LoggerFrom(f.ctx).Errorw("Failed to upsert upstreamGroup due to resource conflict", zap.Error(err))
			statusBuilder.AddDestination(cluster, desiredUpstreamGroup.Namespace, mc_types.PlacementStatus_Namespace{
				State:   mc_types.PlacementStatus_FAILED,
				Message: placement.FailedToUpsertResourceDueToConflict("upstreamGroup"),
			})
		} else if err != nil {
			multiErr.Errors = append(multiErr.Errors, err)
			contextutils.LoggerFrom(f.ctx).Errorw("Failed to upsert upstreamGroup", zap.Error(err))
			statusBuilder.AddDestination(cluster, desiredUpstreamGroup.Namespace, mc_types.PlacementStatus_Namespace{
				State:   mc_types.PlacementStatus_FAILED,
				Message: placement.FailedToUpsertResource("upstreamGroup"),
			})
		} else {
			statusBuilder.AddDestination(cluster, desiredUpstreamGroup.Namespace, mc_types.PlacementStatus_Namespace{
				State: mc_types.PlacementStatus_PLACED,
			})
		}
	}

	for _, staleUpstreamGroup := range existing.Difference(desired).List() {
		err := upstreamGroupClient.DeleteUpstreamGroup(f.ctx, client.ObjectKey{
			Namespace: staleUpstreamGroup.Namespace,
			Name:      staleUpstreamGroup.Name,
		})
		if client.IgnoreNotFound(err) != nil {
			contextutils.LoggerFrom(f.ctx).Errorw("Failed to delete upstreamGroup", zap.Error(err))
			statusBuilder.AddDestination(cluster, staleUpstreamGroup.Namespace, mc_types.PlacementStatus_Namespace{
				State:   mc_types.PlacementStatus_STALE,
				Message: placement.FailedToDeleteResource("upstreamGroup"),
			})
		}
	}

	return multiErr.ErrorOrNil()
}

// Delete all UpstreamGroups managed by the given FederatedUpstreamGroup on all clusters.
// Used to ensure that UpstreamGroups generated by a FederatedUpstreamGroup are cleaned up on delete.
func (f *federatedUpstreamGroupReconciler) deleteAll(fedResource *fed_gloo_solo_io_v1.FederatedUpstreamGroup) error {
	ownerLabel := federation.GetOwnerLabel(fedResource)
	for _, cluster := range f.clusterSet.ListClusters() {
		clusterClient, err := f.baseClients.Cluster(cluster)
		if err != nil {
			return err
		}
		// TODO this requires permissions in all namespaces, we could restrict to to namespaces referenced by gloo instances
		list, err := clusterClient.UpstreamGroups().ListUpstreamGroup(f.ctx, client.MatchingLabels(ownerLabel))
		if err != nil {
			return err
		}

		for _, upstreamGroup := range list.Items {
			// double-check that this UpstreamGroup is actually owned by this FederatedUpstreamGroup
			if !f.verifyOwner(fedResource, upstreamGroup) {
				continue
			}
			err = clusterClient.UpstreamGroups().DeleteUpstreamGroup(f.ctx, client.ObjectKey{
				Namespace: upstreamGroup.Namespace,
				Name:      upstreamGroup.Name,
			})
			if client.IgnoreNotFound(err) != nil {
				return err
			}
		}
	}
	return nil
}

// Ensures that the given federated resource is the owner of the given resource, by checking that the owner annotation on the resource matches the federated resource.
func (f *federatedUpstreamGroupReconciler) verifyOwner(fedResource *fed_gloo_solo_io_v1.FederatedUpstreamGroup, resource gloo_solo_io_v1.UpstreamGroup) bool {
	fedResourceId := federation.GetIdentifier(fedResource)
	// Get the value of the owner annotation on the given resource.
	resourceOwner := resource.GetAnnotations()[federation.HubOwner]
	// This value may be empty (e.g. in earlier versions of Gloo Fed before we added this annotation), so in that case return true.
	// This is safe because before we added the annotation, we always persisted the entire identifier (no truncation) so as long as the label matches,
	// then it's a verified owner match.
	if resourceOwner == "" {
		return true
	}
	return resourceOwner == fedResourceId
}

type federatedSettingsReconciler struct {
	ctx               context.Context
	federatedSettings fed_gloo_solo_io_v1.FederatedSettingsClient
	baseClients       gloo_solo_io_v1.MulticlusterClientset
	placementManager  placement.Manager
	clusterSet        multicluster.ClusterSet
}

func NewFederatedSettingsReconciler(
	ctx context.Context,
	federatedSettings fed_gloo_solo_io_v1.FederatedSettingsClient,
	baseClients gloo_solo_io_v1.MulticlusterClientset,
	placementManager placement.Manager,
	clusterSet multicluster.ClusterSet,
) controller.FederatedSettingsFinalizer {
	return &federatedSettingsReconciler{
		ctx:               ctx,
		federatedSettings: federatedSettings,
		baseClients:       baseClients,
		placementManager:  placementManager,
		clusterSet:        clusterSet,
	}
}

func (f *federatedSettingsReconciler) ReconcileFederatedSettings(obj *fed_gloo_solo_io_v1.FederatedSettings) (reconcile.Result, error) {
	currentPlacementStatus := f.placementManager.GetPlacementStatus(&obj.Status)
	needsReconcile := obj.NeedsReconcile(currentPlacementStatus)
	allClusters := f.clusterSet.ListClusters()
	contextutils.LoggerFrom(f.ctx).Debugw("ReconcileFederatedSettings", zap.Any("FederatedSettings", obj), zap.Any("needsReconcile", needsReconcile),
		zap.Any("allClusters", allClusters))

	if !needsReconcile {
		return reconcile.Result{}, nil
	}

	statusBuilder := f.placementManager.GetBuilder()

	// Validate resource
	if obj.Spec.GetPlacement() == nil {
		updatedPlacementStatus := statusBuilder.
			UpdateUnprocessed(currentPlacementStatus, placement.PlacementMissing, mc_types.PlacementStatus_INVALID).
			Eject(obj.GetGeneration())
		f.placementManager.SetPlacementStatus(&obj.Status, updatedPlacementStatus)
		return reconcile.Result{}, f.federatedSettings.UpdateFederatedSettingsStatus(f.ctx, obj)
	}
	for _, cluster := range obj.Spec.Placement.GetClusters() {
		if !stringutils.ContainsString(cluster, allClusters) {
			updatedPlacementStatus := statusBuilder.
				UpdateUnprocessed(currentPlacementStatus, placement.ClusterNotRegistered(cluster), mc_types.PlacementStatus_INVALID).
				Eject(obj.GetGeneration())
			f.placementManager.SetPlacementStatus(&obj.Status, updatedPlacementStatus)
			return reconcile.Result{}, f.federatedSettings.UpdateFederatedSettingsStatus(f.ctx, obj)
		}
	}
	if obj.Spec.Template.GetSpec() == nil {
		updatedPlacementStatus := statusBuilder.
			UpdateUnprocessed(currentPlacementStatus, placement.SpecTemplateMissing, mc_types.PlacementStatus_INVALID).
			Eject(obj.GetGeneration())
		f.placementManager.SetPlacementStatus(&obj.Status, updatedPlacementStatus)
		return reconcile.Result{}, f.federatedSettings.UpdateFederatedSettingsStatus(f.ctx, obj)
	}
	if obj.Spec.Template.GetMetadata() == nil {
		updatedPlacementStatus := statusBuilder.
			UpdateUnprocessed(currentPlacementStatus, placement.MetaTemplateMissing, mc_types.PlacementStatus_INVALID).
			Eject(obj.GetGeneration())
		f.placementManager.SetPlacementStatus(&obj.Status, updatedPlacementStatus)
		return reconcile.Result{}, f.federatedSettings.UpdateFederatedSettingsStatus(f.ctx, obj)
	}

	// ownerLabel is used to reference Federated resources via their children.
	ownerLabel := federation.GetOwnerLabel(obj)
	// since ownerLabel may be truncated (max length 63), we also store the full value in an annotation
	// for verification in case of collisions
	ownerAnnotation := federation.GetOwnerAnnotation(obj)

	spec := obj.Spec.Template.GetSpec()
	meta := obj.Spec.Template.GetMetadata()
	labels := federation.Merge(meta.GetLabels(), ownerLabel)
	annotations := federation.Merge(meta.GetAnnotations(), ownerAnnotation)

	multiErr := &multierror.Error{}
	for _, cluster := range allClusters {
		clusterSettingss := gloo_solo_io_v1_sets.NewSettingsSet()
		if stringutils.ContainsString(cluster, obj.Spec.Placement.GetClusters()) {
			for _, namespace := range obj.Spec.Placement.GetNamespaces() {

				clusterSettingss.Insert(&gloo_solo_io_v1.Settings{
					ObjectMeta: metav1.ObjectMeta{
						Namespace:   namespace,
						Name:        meta.GetName(),
						Labels:      labels,
						Annotations: annotations,
					},
					Spec: *spec,
				})
			}
		}

		if err := f.ensureCluster(cluster, statusBuilder, clusterSettingss, obj); err != nil {
			multiErr.Errors = append(multiErr.Errors, err)
		}
	}

	f.placementManager.SetPlacementStatus(&obj.Status, statusBuilder.Build(obj.GetGeneration()))
	err := f.federatedSettings.UpdateFederatedSettingsStatus(f.ctx, obj)
	if err != nil {
		multiErr.Errors = append(multiErr.Errors, err)
		contextutils.LoggerFrom(f.ctx).Errorw("Failed to update status on federated settings", zap.Error(err))
	}

	return reconcile.Result{}, multiErr.ErrorOrNil()
}

func (f *federatedSettingsReconciler) FederatedSettingsFinalizerName() string {
	return federation.HubFinalizer
}

func (f *federatedSettingsReconciler) FinalizeFederatedSettings(obj *fed_gloo_solo_io_v1.FederatedSettings) error {
	return f.deleteAll(obj)
}

// ensureCluster upserts all desired resources on the given cluster.
// An error is returned only if a retry is expected to resolve the issue.
func (f *federatedSettingsReconciler) ensureCluster(cluster string, statusBuilder placement.StatusBuilder, desired gloo_solo_io_v1_sets.SettingsSet, fedResource *fed_gloo_solo_io_v1.FederatedSettings) error {
	clientset, err := f.baseClients.Cluster(cluster)
	if err != nil {
		var namespaces []string
		for _, obj := range desired.List() {
			namespaces = append(namespaces, obj.GetNamespace())
		}

		contextutils.LoggerFrom(f.ctx).Errorw("Failed to get clientset", zap.String("cluster", cluster), zap.Error(err))
		statusBuilder.AddDestinations([]string{cluster}, namespaces, mc_types.PlacementStatus_Namespace{
			State:   mc_types.PlacementStatus_FAILED,
			Message: placement.FailedToCreateClientForCluster(cluster),
		})
		return err
	}

	settingsClient := clientset.Settings()

	ownerLabel := federation.GetOwnerLabel(fedResource)
	existingList, err := settingsClient.ListSettings(f.ctx, client.MatchingLabels(ownerLabel))
	if err != nil {
		var namespaces []string
		for _, obj := range desired.List() {
			namespaces = append(namespaces, obj.GetNamespace())
		}

		contextutils.LoggerFrom(f.ctx).Errorw("Failed to list Settingss", zap.String("cluster", cluster), zap.Error(err))
		statusBuilder.AddDestinations([]string{cluster}, namespaces, mc_types.PlacementStatus_Namespace{
			State:   mc_types.PlacementStatus_FAILED,
			Message: placement.FailedToListResource("settings", cluster),
		})
		return err
	}

	existing := gloo_solo_io_v1_sets.NewSettingsSet()
	for _, settings := range existingList.Items {
		// double-check that this Settings is actually owned by this FederatedSettings
		if !f.verifyOwner(fedResource, settings) {
			continue
		}
		settingsPointer := settings
		existing.Insert(&settingsPointer)
	}

	multiErr := &multierror.Error{}
	for _, desiredSettings := range desired.List() {
		err := settingsClient.UpsertSettings(f.ctx, desiredSettings)
		if err != nil && errors.IsConflict(err) {
			multiErr.Errors = append(multiErr.Errors, err)
			contextutils.LoggerFrom(f.ctx).Errorw("Failed to upsert settings due to resource conflict", zap.Error(err))
			statusBuilder.AddDestination(cluster, desiredSettings.Namespace, mc_types.PlacementStatus_Namespace{
				State:   mc_types.PlacementStatus_FAILED,
				Message: placement.FailedToUpsertResourceDueToConflict("settings"),
			})
		} else if err != nil {
			multiErr.Errors = append(multiErr.Errors, err)
			contextutils.LoggerFrom(f.ctx).Errorw("Failed to upsert settings", zap.Error(err))
			statusBuilder.AddDestination(cluster, desiredSettings.Namespace, mc_types.PlacementStatus_Namespace{
				State:   mc_types.PlacementStatus_FAILED,
				Message: placement.FailedToUpsertResource("settings"),
			})
		} else {
			statusBuilder.AddDestination(cluster, desiredSettings.Namespace, mc_types.PlacementStatus_Namespace{
				State: mc_types.PlacementStatus_PLACED,
			})
		}
	}

	for _, staleSettings := range existing.Difference(desired).List() {
		err := settingsClient.DeleteSettings(f.ctx, client.ObjectKey{
			Namespace: staleSettings.Namespace,
			Name:      staleSettings.Name,
		})
		if client.IgnoreNotFound(err) != nil {
			contextutils.LoggerFrom(f.ctx).Errorw("Failed to delete settings", zap.Error(err))
			statusBuilder.AddDestination(cluster, staleSettings.Namespace, mc_types.PlacementStatus_Namespace{
				State:   mc_types.PlacementStatus_STALE,
				Message: placement.FailedToDeleteResource("settings"),
			})
		}
	}

	return multiErr.ErrorOrNil()
}

// Delete all Settingss managed by the given FederatedSettings on all clusters.
// Used to ensure that Settingss generated by a FederatedSettings are cleaned up on delete.
func (f *federatedSettingsReconciler) deleteAll(fedResource *fed_gloo_solo_io_v1.FederatedSettings) error {
	ownerLabel := federation.GetOwnerLabel(fedResource)
	for _, cluster := range f.clusterSet.ListClusters() {
		clusterClient, err := f.baseClients.Cluster(cluster)
		if err != nil {
			return err
		}
		// TODO this requires permissions in all namespaces, we could restrict to to namespaces referenced by gloo instances
		list, err := clusterClient.Settings().ListSettings(f.ctx, client.MatchingLabels(ownerLabel))
		if err != nil {
			return err
		}

		for _, settings := range list.Items {
			// double-check that this Settings is actually owned by this FederatedSettings
			if !f.verifyOwner(fedResource, settings) {
				continue
			}
			err = clusterClient.Settings().DeleteSettings(f.ctx, client.ObjectKey{
				Namespace: settings.Namespace,
				Name:      settings.Name,
			})
			if client.IgnoreNotFound(err) != nil {
				return err
			}
		}
	}
	return nil
}

// Ensures that the given federated resource is the owner of the given resource, by checking that the owner annotation on the resource matches the federated resource.
func (f *federatedSettingsReconciler) verifyOwner(fedResource *fed_gloo_solo_io_v1.FederatedSettings, resource gloo_solo_io_v1.Settings) bool {
	fedResourceId := federation.GetIdentifier(fedResource)
	// Get the value of the owner annotation on the given resource.
	resourceOwner := resource.GetAnnotations()[federation.HubOwner]
	// This value may be empty (e.g. in earlier versions of Gloo Fed before we added this annotation), so in that case return true.
	// This is safe because before we added the annotation, we always persisted the entire identifier (no truncation) so as long as the label matches,
	// then it's a verified owner match.
	if resourceOwner == "" {
		return true
	}
	return resourceOwner == fedResourceId
}
