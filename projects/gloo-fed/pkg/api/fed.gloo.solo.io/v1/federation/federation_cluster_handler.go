// Code generated by skv2. DO NOT EDIT.

// Definition for federated resource cluster handler templates
package federation

import (
	"context"

	"github.com/avast/retry-go/v4"
	"github.com/solo-io/go-utils/contextutils"
	"github.com/solo-io/skv2/pkg/multicluster"
	fed_gloo_solo_io_v1 "github.com/solo-io/solo-projects/projects/gloo-fed/pkg/api/fed.gloo.solo.io/v1"
	mc_types "github.com/solo-io/solo-projects/projects/gloo-fed/pkg/api/fed.solo.io/core/v1"
	"github.com/solo-io/solo-projects/projects/gloo-fed/pkg/federation"
	"github.com/solo-io/solo-projects/projects/gloo-fed/pkg/federation/placement"
	"go.uber.org/zap"
	"k8s.io/apimachinery/pkg/api/errors"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/manager"
)

type clusterHandler struct {
	ctx     context.Context
	clients fed_gloo_solo_io_v1.Clientset
	manager placement.Manager
}

func NewClusterHandler(ctx context.Context, clients fed_gloo_solo_io_v1.Clientset, manager placement.Manager) multicluster.ClusterHandler {
	return &clusterHandler{
		ctx:     ctx,
		clients: clients,
		manager: manager,
	}
}

func (f *clusterHandler) AddCluster(_ context.Context, cluster string, _ manager.Manager) {
	f.handleClusterEvent(cluster)
}

func (f *clusterHandler) RemoveCluster(cluster string) {
	f.handleClusterEvent(cluster)
}

func (f *clusterHandler) handleClusterEvent(cluster string) {

	federatedUpstreamList, err := f.clients.FederatedUpstreams().ListFederatedUpstream(f.ctx)
	if err != nil {
		contextutils.LoggerFrom(f.ctx).Errorf("Failed to list FederatedUpstreams referencing cluster %s", cluster)
	} else {
		for _, item := range federatedUpstreamList.Items {
			item := item
			if err := f.maybeUpdateFederatedUpstreamStatusWithRetries(&item, cluster); err != nil {
				contextutils.LoggerFrom(f.ctx).Errorw("Failed to update status on FederatedUpstream",
					zap.Error(err),
					zap.Any("FederatedUpstream", item))
			}
		}
	}

	federatedUpstreamGroupList, err := f.clients.FederatedUpstreamGroups().ListFederatedUpstreamGroup(f.ctx)
	if err != nil {
		contextutils.LoggerFrom(f.ctx).Errorf("Failed to list FederatedUpstreamGroups referencing cluster %s", cluster)
	} else {
		for _, item := range federatedUpstreamGroupList.Items {
			item := item
			if err := f.maybeUpdateFederatedUpstreamGroupStatusWithRetries(&item, cluster); err != nil {
				contextutils.LoggerFrom(f.ctx).Errorw("Failed to update status on FederatedUpstreamGroup",
					zap.Error(err),
					zap.Any("FederatedUpstreamGroup", item))
			}
		}
	}

	federatedSettingsList, err := f.clients.FederatedSettings().ListFederatedSettings(f.ctx)
	if err != nil {
		contextutils.LoggerFrom(f.ctx).Errorf("Failed to list FederatedSettingss referencing cluster %s", cluster)
	} else {
		for _, item := range federatedSettingsList.Items {
			item := item
			if err := f.maybeUpdateFederatedSettingsStatusWithRetries(&item, cluster); err != nil {
				contextutils.LoggerFrom(f.ctx).Errorw("Failed to update status on FederatedSettings",
					zap.Error(err),
					zap.Any("FederatedSettings", item))
			}
		}
	}
}

func (f *clusterHandler) maybeUpdateFederatedUpstreamStatusWithRetries(item *fed_gloo_solo_io_v1.FederatedUpstream, cluster string) error {
	return retry.Do(func() error {
		err := f.maybeUpdateFederatedUpstreamStatus(item, cluster)
		if err != nil && errors.IsNotFound(err) {
			// If the resource no longer exists, there is nothing to do.
			return nil
		} else if err != nil {
			// On conflict, retry with the new object to pick up any changes to the resource's spec.
			obj, err := f.clients.FederatedUpstreams().GetFederatedUpstream(f.ctx, client.ObjectKey{Namespace: item.Namespace, Name: item.Name})
			if err != nil {
				return err
			}
			item = obj
		}
		return err
	}, federation.GetClusterWatcherLocalRetryOptions(f.ctx)...)
}

func (f *clusterHandler) maybeUpdateFederatedUpstreamStatus(item *fed_gloo_solo_io_v1.FederatedUpstream, cluster string) error {
	for _, c := range item.Spec.Placement.GetClusters() {
		if c == cluster {
			currentPlacementStatus := f.manager.GetPlacementStatus(&item.Status)

			// An existing resource references the given cluster. Update its status to trigger a resync.
			updatedPlacementStatus := f.manager.GetBuilder().
				UpdateUnprocessed(currentPlacementStatus, placement.ClusterEventTriggered(cluster), mc_types.PlacementStatus_PENDING).
				// Do not update the observed generation or written by fields as we have not actually processed the resource.
				Eject(currentPlacementStatus.GetObservedGeneration())
			f.manager.SetPlacementStatus(&item.Status, updatedPlacementStatus)

			return f.clients.FederatedUpstreams().UpdateFederatedUpstreamStatus(f.ctx, item)
		}
	}
	return nil
}

func (f *clusterHandler) maybeUpdateFederatedUpstreamGroupStatusWithRetries(item *fed_gloo_solo_io_v1.FederatedUpstreamGroup, cluster string) error {
	return retry.Do(func() error {
		err := f.maybeUpdateFederatedUpstreamGroupStatus(item, cluster)
		if err != nil && errors.IsNotFound(err) {
			// If the resource no longer exists, there is nothing to do.
			return nil
		} else if err != nil {
			// On conflict, retry with the new object to pick up any changes to the resource's spec.
			obj, err := f.clients.FederatedUpstreamGroups().GetFederatedUpstreamGroup(f.ctx, client.ObjectKey{Namespace: item.Namespace, Name: item.Name})
			if err != nil {
				return err
			}
			item = obj
		}
		return err
	}, federation.GetClusterWatcherLocalRetryOptions(f.ctx)...)
}

func (f *clusterHandler) maybeUpdateFederatedUpstreamGroupStatus(item *fed_gloo_solo_io_v1.FederatedUpstreamGroup, cluster string) error {
	for _, c := range item.Spec.Placement.GetClusters() {
		if c == cluster {
			currentPlacementStatus := f.manager.GetPlacementStatus(&item.Status)

			// An existing resource references the given cluster. Update its status to trigger a resync.
			updatedPlacementStatus := f.manager.GetBuilder().
				UpdateUnprocessed(currentPlacementStatus, placement.ClusterEventTriggered(cluster), mc_types.PlacementStatus_PENDING).
				// Do not update the observed generation or written by fields as we have not actually processed the resource.
				Eject(currentPlacementStatus.GetObservedGeneration())
			f.manager.SetPlacementStatus(&item.Status, updatedPlacementStatus)

			return f.clients.FederatedUpstreamGroups().UpdateFederatedUpstreamGroupStatus(f.ctx, item)
		}
	}
	return nil
}

func (f *clusterHandler) maybeUpdateFederatedSettingsStatusWithRetries(item *fed_gloo_solo_io_v1.FederatedSettings, cluster string) error {
	return retry.Do(func() error {
		err := f.maybeUpdateFederatedSettingsStatus(item, cluster)
		if err != nil && errors.IsNotFound(err) {
			// If the resource no longer exists, there is nothing to do.
			return nil
		} else if err != nil {
			// On conflict, retry with the new object to pick up any changes to the resource's spec.
			obj, err := f.clients.FederatedSettings().GetFederatedSettings(f.ctx, client.ObjectKey{Namespace: item.Namespace, Name: item.Name})
			if err != nil {
				return err
			}
			item = obj
		}
		return err
	}, federation.GetClusterWatcherLocalRetryOptions(f.ctx)...)
}

func (f *clusterHandler) maybeUpdateFederatedSettingsStatus(item *fed_gloo_solo_io_v1.FederatedSettings, cluster string) error {
	for _, c := range item.Spec.Placement.GetClusters() {
		if c == cluster {
			currentPlacementStatus := f.manager.GetPlacementStatus(&item.Status)

			// An existing resource references the given cluster. Update its status to trigger a resync.
			updatedPlacementStatus := f.manager.GetBuilder().
				UpdateUnprocessed(currentPlacementStatus, placement.ClusterEventTriggered(cluster), mc_types.PlacementStatus_PENDING).
				// Do not update the observed generation or written by fields as we have not actually processed the resource.
				Eject(currentPlacementStatus.GetObservedGeneration())
			f.manager.SetPlacementStatus(&item.Status, updatedPlacementStatus)

			return f.clients.FederatedSettings().UpdateFederatedSettingsStatus(f.ctx, item)
		}
	}
	return nil
}
